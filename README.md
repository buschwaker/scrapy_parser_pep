[![](https://img.shields.io/badge/Scrapy-2.5.1-orange)](https://img.shields.io/badge/Scrapy-2.5.1-orange)
[![](https://img.shields.io/badge/Python-3.7.0-green)](https://img.shields.io/badge/Python-3.7.0-green)

# Асинхронный парсер PEP

## Описание

Создан парсер <a href ="https://peps.python.org/">документов PEP</a> на базе фреймворка Scrapy.

1. В первый файл выводится список всех PEP: номер, название и статус.
2. Во второй файл выводится сводка по статусам PEP — сколько найдено документов в каждом статусе (статус, количество). В последней строке этого файла в колонке «Статус» стоит слово Total, а в колонке «Количество» — общее количество всех документов.

## Структура проекта

1. В качестве стартовой ссылки паука установлено https://peps.python.org/. Метод паука `parse()` собирает ссылки на документы PEP. Метод `parse_pep()` парсит страницы с документами и формирует Items.
2. Для создания Items описан класс `PepParseItem(scrapy.Item)`, у него три атрибута:
   - `number` (номер PEP),
   - `name` (название PEP),
   - `status` (статус, указанный на странице PEP)
3. Парсер сохраняет данные в файлы .csv в директорию results/:
   - **Файлы со списком PEP** именованы по маске pep_ДатаВремя.cs. В файле формируется три столбца: `«Номер»`, `«Название»` и `«Статус»`. Сохранение выполняется посредством Feeds.
   - **Файлы со сводкой по статусам** именованы по маске status_summary_ДатаВремя.csv. Создается этот файл через Pipeline. В файле два столбца: `«Статус» и «Количество»`. В конце записывается общее число PEP под ключом `Total`.

## Инструкция по запуску проекта на своей машине:
1. Скачиваем репозиторий
2. Устанавливаем и активируем виртуальное окружение  
3. Устанавливаем зависимости `pip install -r requirements.txt`
4. Запускается парсер командой `scrapy crawl pep`